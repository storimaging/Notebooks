{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/luciabouzaheguerte/NotebooksProject/blob/master/ImageGeneration/Energy_based_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUaar2Ku_nm-"
   },
   "source": [
    "# Normalizing flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12HQg0ZeC7xV"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This practical session explains how to use and train normalizing flows (NF) as generative models. Specifically, we will use NF to generate images as MNIST digits. \n",
    "\n",
    "**References:**\n",
    "\n",
    "This practical session is based on several resources:\n",
    "\n",
    "* Valentin De Bortoli's course: https://vdeborto.github.io/project/generative_modeling/session_1.pdf \n",
    "* Model, code and tutorial from Philipp Lippe: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial11/NF_image_modeling.html\n",
    "\n",
    "\n",
    "**Authors:**\n",
    "* Lucía Bouza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvvDOJqgCMKI"
   },
   "source": [
    "## Underlying principle\n",
    "\n",
    "Supose we want to estimate the probability distribution $p(x)$ of a image dataset. Energy-based models allow converting any function with output values greater than zero, into a probability distribution, dividing it by its volume.\n",
    "\n",
    "If $f_\\Theta(x)$ is the output of a neural network where $\\Theta$ are the parameters and $x$ is the input image, we can define a probability distribution $q_\\Theta(x)$ as shown below. $q_\\Theta(x)$ will be the function learned by the model, which will be trained to approximate $p(x)$. \n",
    "\n",
    "$$ q_\\Theta(x) = \\frac{exp(-f_\\Theta(x))}{Z_\\Theta} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{ where } Z_\\Theta = \\int_x exp(-f_\\Theta(x)) dx \n",
    "\\text{ (considering $x$ continuous)}\n",
    "$$\n",
    "\n",
    "Compute $Z_\\Theta$ analytically is not possible, so another training method is used for EBM. Contrastive Divergence is a training technique that consists on compare the likelihood of samples, and try to minimize the energy for samples from the dataset, while maximizing the energy for samples from the model. Markov Chain Monte Carlo is used for sampling from the EBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R44_-oj9BSQV"
   },
   "source": [
    "## Importing packages\n",
    "\n",
    "Below is a list of packages needed to implement normalizing flows. PyTorch version used to run this notebook is **1.11.0+cu113** (to check the installed version, use `torch.__version__`)\n",
    "\n",
    "* `torch` (indispensable packages for neural networks with PyTorch)\n",
    "* `nn` (to define network)\n",
    "* `MNIST`, `transforms` and `data` (to work with the MNIST dataset)\n",
    "* `matplotlib.pyplot` (to display images)\n",
    "* `sample` (to sample from an array)\n",
    "* `os` (to interact with the operating system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JeRxkMzR_giv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M2ifv39CY4s"
   },
   "source": [
    "## Set a device\n",
    "\n",
    "Next, we need to choose which device to run the algorithm on. We can use `torch.cuda.is_available()` to detect if there is a GPU available. Next, we set the `torch.device`. The `.to(device)` method is used to move tensors or modules to a desired device, we will use it in next sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvVDfS-SCbvN"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\", device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFZo7OPRB5JK"
   },
   "source": [
    "## Model\n",
    "\n",
    "We will generate images as MNIST digits with a simple CNN model.\n",
    "In this notebook, for the sake of simplicity (and computing time!), we will use a pre-trained model from [Philipp Lippe's tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html). \n",
    "\n",
    "It's important to know that EBM are not easy to train and often diverge if the hyperparameters are not well tuned. The downloaded model was training relying on training tricks proposed in the paper [Implicit Generation and Generalization in Energy-Based Models](https://arxiv.org/abs/1903.08689) by Yilun Du and Igor Mordatch ([blog](https://openai.com/blog/energy-based-models/)).\n",
    "\n",
    "\n",
    "We will download the model on the next cell and then define the CNN network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V39ZEXZ5JXlK"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial8/MNIST.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLsZaCCiLmhB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code from Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "\"\"\"\n",
    "\n",
    "# smooth activation function\n",
    "class Swish(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_features=32, out_dim=1, **kwargs):\n",
    "        super().__init__()\n",
    "        # We increase the hidden dimension over layers. Here pre-calculated for simplicity.\n",
    "        c_hid1 = hidden_features//2\n",
    "        c_hid2 = hidden_features\n",
    "        c_hid3 = hidden_features*2\n",
    "        \n",
    "        # Series of convolutions and Swish activation functions\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "                nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4), # [16x16] - Larger padding to get 32x32 image\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1), #  [8x8]\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1), # [4x4]\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid3, c_hid3, kernel_size=3, stride=2, padding=1), # [2x2]\n",
    "                Swish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(c_hid3*4, c_hid3),\n",
    "                Swish(),\n",
    "                nn.Linear(c_hid3, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x).squeeze(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dgz0DJtCNlBf"
   },
   "source": [
    "Next we will load the model (we need to crop the key names, because the model was saved on a different way as we will use it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMT-gedbNmm6",
    "outputId": "e678ba34-2cff-4abd-c230-58d1e4d0bc12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"MNIST.ckpt\", map_location=torch.device(device))\n",
    "\n",
    "state_dict = checkpoint['state_dict'].copy()\n",
    "for k in checkpoint['state_dict'].keys():\n",
    "   state_dict[k[4:]] = state_dict.pop(k)\n",
    "\n",
    "model = CNNModel()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiLFjHvJKJat"
   },
   "source": [
    "## Training Steps\n",
    "\n",
    "Although we will not run the training on this notebook, it is interesting to see the steps required to do so. The main steps are:\n",
    "\n",
    "1.  Download the dataset.\n",
    "2.  Define how to perform fake images sampling during training.\n",
    "3.  Run the training loop defined in the following pseudocode:\n",
    "    - Start with empty buffer.\n",
    "    - For each step do:\n",
    "      - Sampling real data from the dataset.\n",
    "      - Sampling fake data with the procedure defined previously.\n",
    "      - Add the new samples to the buffer.\n",
    "      - Calculate the contrastive divergence objective using the energy model.\n",
    "      - Add a regularization loss on the model output (to make sure that the output values are within a reasonable range). \n",
    "      - Perform an update step with an optimizer on the combined loss (use a weight factor α to give more importance to the Contrastive Divergence against regularization loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WJCX8RvcTiX"
   },
   "source": [
    "### 1. Download MNIST dataset\n",
    "\n",
    "Note that we normalize the images between -1 and 1 because during sampling, we have to limit the input space and scaling between -1 and 1 makes it easier to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3hfFYJkPdy1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code based on Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "\"\"\"\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])\n",
    "train_set = MNIST(os.getcwd(), train=True, transform=transform, download=True)\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True,  drop_last=True,  num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNuSu4pJ1wgx"
   },
   "source": [
    "### 2. How to sample images during training\n",
    "\n",
    "To get quality samples, we need to run many iterations of MCMC. To reduce cost and execution time, we buffer the samples from previous runs and use them as a starting point to generate new samples. this reduces the number of iterations required to obtain quality samples. In addition, in order not to lose the exploratory capacity, 5% of the samples are obtained based on white noise, and not from an image of the buffer.\n",
    "\n",
    "Function `sample_new_exmps` takes 5% noise-based images, and 95% buffer images, and applies num_steps iterations to each MCMC image to obtain a batch of generated images. Function `generate_samples` implements MCMC sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BRy4fx9N5zVQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code from Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "Small changes were done to the original code\n",
    "\"\"\"\n",
    "\n",
    "class Sampler:\n",
    "\n",
    "    def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            img_shape - Shape of the images to model\n",
    "            sample_size - Batch size of the samples\n",
    "            max_len - Maximum number of data points to keep in the buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.img_shape = img_shape\n",
    "        self.sample_size = sample_size\n",
    "        self.max_len = max_len\n",
    "        self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
    "\n",
    "    def sample_new_exmps(self, steps=60, step_size=10):\n",
    "        \"\"\"\n",
    "        Function for getting a new batch of \"fake\" images.\n",
    "        Inputs:\n",
    "            steps - Number of iterations in the MCMC algorithm\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "        \"\"\"\n",
    "        # Choose 95% of the batch from the buffer, 5% generate from scratch\n",
    "        n_new = int(0.05*self.sample_size)\n",
    "        indexes_buffer = sample(range(self.sample_size), self.sample_size-n_new)\n",
    "        examples = [self.examples[i] for i in indexes_buffer]\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n",
    "        old_imgs = torch.cat(examples, dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)\n",
    "\n",
    "        # Perform MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
    "\n",
    "        # Add new images to the buffer and remove old ones if needed\n",
    "        self.examples = list(inp_imgs.to(torch.device(\"cpu\")).chunk(self.sample_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size=10):\n",
    "        \"\"\"\n",
    "        Function for sampling images for a given model. \n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
    "            steps - Number of iterations in the MCMC algorithm.\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "        \"\"\"\n",
    "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
    "        # because we are only interested in the gradients of the input. \n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        inp_imgs.requires_grad = True\n",
    "        \n",
    "        # Enable gradient calculation if not already the case\n",
    "        had_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
    "        # More efficient than creating a new tensor every iteration.\n",
    "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
    "        \n",
    "        # Loop over K (steps)\n",
    "        for _ in range(steps):\n",
    "            # Part 1: Add noise to the input.\n",
    "            noise.normal_(0, 0.005)\n",
    "            inp_imgs.data.add_(noise.data)\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "            \n",
    "            # Part 2: calculate gradients for the current input.\n",
    "            out_imgs = -model(inp_imgs)\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
    "\n",
    "            # Apply gradients to our current samples\n",
    "            inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "        \n",
    "        # Reactivate gradients for parameters for training\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        model.train(is_training)\n",
    "        \n",
    "        # Reset gradient calculation to setting before this function\n",
    "        torch.set_grad_enabled(had_gradients_enabled)\n",
    "\n",
    "        return inp_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suRpsXWzFlub"
   },
   "source": [
    "### 3. Training loop\n",
    "\n",
    "In the next section we define the training loop as shown in the pseudocode above. The hyperparameters defined here are specific to the MNIST dataset and were fine-tuned by Philipp Lippe in the tutorial on which this notebook is based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u6PR2ZMXE-1M"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code based on Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.0, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97) # Exponential decay over epochs\n",
    "\n",
    "def training_loop(cnn, train_loader, optimizer, scheduler, num_epochs = 60, alpha=0.1, img_shape=(1,28,28)):\n",
    "\n",
    "  sampler = Sampler(cnn, img_shape=img_shape, sample_size=train_loader.batch_size)\n",
    "\n",
    "  # Put model to train mode\n",
    "  cnn.train()\n",
    "  torch.set_grad_enabled(True)\n",
    "\n",
    "  for epochs in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Sampling real data from the dataset\n",
    "        # (Add minimal noise to the original images to prevent the model from focusing on purely \"clean\" inputs)\n",
    "        real_imgs, _ = batch\n",
    "        small_noise = torch.randn_like(real_imgs) * 0.005\n",
    "        real_imgs.add_(small_noise).clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "        # 2. Sampling fake data and add the new samples to the buffer.\n",
    "        fake_imgs = sampler.sample_new_exmps(steps=60, step_size=10)\n",
    "\n",
    "        # 3. calculate the contrastive divergence objective\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = cnn(inp_imgs).chunk(2, dim=0)\n",
    "        cdiv_loss = fake_out.mean() - real_out.mean()\n",
    "\n",
    "        # 4. Add a regularization loss\n",
    "        reg_loss = (real_out ** 2 + fake_out ** 2).mean()\n",
    "      \n",
    "        # 5. Perform an update step with an optimizer on the combined loss\n",
    "        loss = alpha * reg_loss + cdiv_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsWyhXtnKoSC"
   },
   "source": [
    "To run the training, execute the cell below. Consider having the necessary resources where you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3VVtAV3eIhoe"
   },
   "outputs": [],
   "source": [
    "#training_loop(model, train_loader, optimizer, scheduler, num_epochs = 1, alpha=0.1, img_shape=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QABdS5DF4OEB"
   },
   "source": [
    "## Image generation\n",
    "\n",
    "In this section we will sample images from the model to check if they are as realistic as those of MNIST dataset. This will show if we have successfully modeled the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "NIrYTjqh5Xo9",
    "outputId": "2d1c16dc-65b7-48fa-8323-2aa0f97a0eef"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def printResults(imgs):\n",
    "    \"\"\" Displays sample images\n",
    "    \"\"\" \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(8, 8))\n",
    "    axes[0].imshow(imgs[0][0,:,:], cmap='gray')\n",
    "    axes[1].imshow(imgs[1][0,:,:], cmap='gray')\n",
    "    axes[2].imshow(imgs[2][0,:,:], cmap='gray')\n",
    "    axes[3].imshow(imgs[3][0,:,:], cmap='gray')\n",
    "    fig.tight_layout()\n",
    "\n",
    "n_images_to_generate = 4\n",
    "start_imgs = (torch.rand((n_images_to_generate,) + (1,28,28))*2-1).to(device)\n",
    "imgs = Sampler.generate_samples(model, start_imgs, steps=256, step_size=10)\n",
    "printResults(imgs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNH1Yt6aMwA/71r5TzYHbaR",
   "include_colab_link": true,
   "name": "Normalizing flows.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
