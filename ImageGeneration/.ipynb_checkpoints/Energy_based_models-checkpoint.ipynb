{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/luciabouzaheguerte/NotebooksProject/blob/master/ImageGeneration/Energy_based_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUaar2Ku_nm-"
   },
   "source": [
    "# Energy based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "12HQg0ZeC7xV"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This practical session explains how to use and train energy-based models (EBM) as generative models. Specifically, we will use EBM to generate images as MNIST digits. \n",
    "\n",
    "**References:**\n",
    "\n",
    "This practical session is based on several resources:\n",
    "\n",
    "* Valentin De Bortoli's course: https://vdeborto.github.io/project/generative_modeling/session_1.pdf \n",
    "* Model, code and tutorial from Philipp Lippe: https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html\n",
    "\n",
    "\n",
    "**Authors:**\n",
    "* Lucía Bouza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvvDOJqgCMKI"
   },
   "source": [
    "## Underlying principle\n",
    "\n",
    "TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R44_-oj9BSQV"
   },
   "source": [
    "## Importing packages\n",
    "\n",
    "Below is a list of packages needed to implement energy-based models. PyTorch version used to run this notebook is **1.11.0+cu113** (to check the installed version, use `torch.__version__`)\n",
    "\n",
    "* `torch` (indispensable packages for neural networks with PyTorch)\n",
    "* `torchvision.models` (to get the vgg network)\n",
    "* `torchvision.transforms.functional` (to transform images into tensors)\n",
    "* `matplotlib.pyplot` (to load and display images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "JeRxkMzR_giv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "import torch.utils.data as data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M2ifv39CY4s"
   },
   "source": [
    "## Set a device\n",
    "\n",
    "Next, we need to choose which device to run the algorithm on. We can use `torch.cuda.is_available()` to detect if there is a GPU available. Next, we set the `torch.device`. The `.to(device)` method is used to move tensors or modules to a desired device, we will use it in next sections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rvVDfS-SCbvN",
    "outputId": "57e82b7d-0f1e-4ceb-ac18-6496b617caf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu\n",
      "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device is\", device)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFZo7OPRB5JK"
   },
   "source": [
    "## Model\n",
    "\n",
    "We will generate images as MNIST digits with a simple CNN model.\n",
    "In this notebook, for the sake of simplicity (and computing time!), we will use a pre-trained model from [Philipp Lippe's tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html). \n",
    "\n",
    "It's important to know that EBM are not easy to train and often diverge if the hyperparameters are not well tuned. The downloaded model was training relying on training tricks proposed in the paper [Implicit Generation and Generalization in Energy-Based Models](https://arxiv.org/abs/1903.08689) by Yilun Du and Igor Mordatch ([blog](https://openai.com/blog/energy-based-models/)).\n",
    "\n",
    "\n",
    "We will download the model on the next cell and then define the CNN network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V39ZEXZ5JXlK",
    "outputId": "084d043c-949e-4150-9e2a-6fed37490baf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-29 11:14:58--  https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial8/MNIST.ckpt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 310651 (303K) [application/octet-stream]\n",
      "Saving to: ‘MNIST.ckpt.3’\n",
      "\n",
      "MNIST.ckpt.3        100%[===================>] 303.37K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2022-06-29 11:14:59 (7.37 MB/s) - ‘MNIST.ckpt.3’ saved [310651/310651]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial8/MNIST.ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "TLsZaCCiLmhB"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code from Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "\"\"\"\n",
    "\n",
    "# smooth activation function\n",
    "class Swish(nn.Module):\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_features=32, out_dim=1, **kwargs):\n",
    "        super().__init__()\n",
    "        # We increase the hidden dimension over layers. Here pre-calculated for simplicity.\n",
    "        c_hid1 = hidden_features//2\n",
    "        c_hid2 = hidden_features\n",
    "        c_hid3 = hidden_features*2\n",
    "        \n",
    "        # Series of convolutions and Swish activation functions\n",
    "        self.cnn_layers = nn.Sequential(\n",
    "                nn.Conv2d(1, c_hid1, kernel_size=5, stride=2, padding=4), # [16x16] - Larger padding to get 32x32 image\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid1, c_hid2, kernel_size=3, stride=2, padding=1), #  [8x8]\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid2, c_hid3, kernel_size=3, stride=2, padding=1), # [4x4]\n",
    "                Swish(),\n",
    "                nn.Conv2d(c_hid3, c_hid3, kernel_size=3, stride=2, padding=1), # [2x2]\n",
    "                Swish(),\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(c_hid3*4, c_hid3),\n",
    "                Swish(),\n",
    "                nn.Linear(c_hid3, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x).squeeze(dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dgz0DJtCNlBf"
   },
   "source": [
    "Next we will load the model (we have to crop the names of the keys, because the model was saved on a different way as we will use it). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMT-gedbNmm6",
    "outputId": "e678ba34-2cff-4abd-c230-58d1e4d0bc12"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load(\"MNIST.ckpt\", map_location=torch.device(device))\n",
    "\n",
    "state_dict = checkpoint['state_dict'].copy()\n",
    "for k in checkpoint['state_dict'].keys():\n",
    "   state_dict[k[4:]] = state_dict.pop(k)\n",
    "\n",
    "model = CNNModel()\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiLFjHvJKJat"
   },
   "source": [
    "## Training Steps\n",
    "\n",
    "Although we do not run the training on this notebook, it is interesting to see the steps required to do so. The main steps are:\n",
    "\n",
    "1.  download the dataset.\n",
    "2.  define how to perform the sampling of fake images during training\n",
    "3.  execute the training loop defined in the following pseudocode:\n",
    "    - start with empty buffer.\n",
    "    - foreach step do:\n",
    "      - sampling real data from the dataset\n",
    "      - sampling fake data with the procedure defined previusly\n",
    "      - calculate the contrastive divergence objective using the energy model.\n",
    "      - add a regularization loss on the output of the model (to ensure that the output values are in a reasonable range). \n",
    "      - perform an update step with an optimizer on the combined loss (As the regularization loss is less important than the Contrastive Divergence, we have a weight factor α which is usually quite some smaller than 1.)\n",
    "      - add the new samples to the buffer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WJCX8RvcTiX"
   },
   "source": [
    "### 1. Download MNIST dataset\n",
    "\n",
    "Note that we normalize the images between -1 and 1 because during sampling, we have to limit the input space and scaling between -1 and 1 makes it easier to implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "D3hfFYJkPdy1"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code from Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "\"\"\"\n",
    "\n",
    "DATASET_PATH = \"../data\"\n",
    "\n",
    "# Transformations applied on each image => make them a tensor and normalize between -1 and 1\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))\n",
    "                               ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "train_set = MNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
    "\n",
    "# Loading the test set\n",
    "test_set = MNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "# We define a set of data loaders that we can use for various purposes later.\n",
    "# Note that for actually training a model, we will use different data loaders\n",
    "# with a lower batch size.\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True,  drop_last=True,  num_workers=2, pin_memory=True)\n",
    "test_loader  = data.DataLoader(test_set,  batch_size=256, shuffle=False, drop_last=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNuSu4pJ1wgx"
   },
   "source": [
    "### 2. How to sample images during training\n",
    "\n",
    "To get quality samples, we need to run many iterations of MCMC. To reduce cost and execution time, we buffer the samples from previous runs and use them as a starting point to generate new samples. this reduces the number of iterations required to obtain quality samples. In addition, in order not to lose the exploratory capacity, 5% of the samples are obtained based on white noise, and not from an image of the buffer.\n",
    "\n",
    "Function `sample_new_exmps` takes 5% noise-based images, and 95% buffer images, and applies num_steps iterations to each MCMC image to obtain a batch of generated images. Function `generate_samples` implements MCMC sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "BRy4fx9N5zVQ"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code from Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "\"\"\"\n",
    "\n",
    "class Sampler:\n",
    "\n",
    "    def __init__(self, model, img_shape, sample_size, max_len=8192):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            img_shape - Shape of the images to model\n",
    "            sample_size - Batch size of the samples\n",
    "            max_len - Maximum number of data points to keep in the buffer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.img_shape = img_shape\n",
    "        self.sample_size = sample_size\n",
    "        self.max_len = max_len\n",
    "        self.examples = [(torch.rand((1,)+img_shape)*2-1) for _ in range(self.sample_size)]\n",
    "\n",
    "    def sample_new_exmps(self, steps=60, step_size=10):\n",
    "        \"\"\"\n",
    "        Function for getting a new batch of \"fake\" images.\n",
    "        Inputs:\n",
    "            steps - Number of iterations in the MCMC algorithm\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "        \"\"\"\n",
    "        # Choose 95% of the batch from the buffer, 5% generate from scratch\n",
    "        n_new = int(0.05*self.sample_size)\n",
    "        indexes_buffer = sample(range(self.sample_size), self.sample_size-n_new)\n",
    "        examples = [self.examples[i] for i in indexes_buffer]\n",
    "        rand_imgs = torch.rand((n_new,) + self.img_shape) * 2 - 1\n",
    "        old_imgs = torch.cat(examples, dim=0)\n",
    "        inp_imgs = torch.cat([rand_imgs, old_imgs], dim=0).detach().to(device)\n",
    "\n",
    "        # Perform MCMC sampling\n",
    "        inp_imgs = Sampler.generate_samples(self.model, inp_imgs, steps=steps, step_size=step_size)\n",
    "\n",
    "        # Add new images to the buffer and remove old ones if needed\n",
    "        self.examples = list(inp_imgs.to(torch.device(\"cpu\")).chunk(self.sample_size, dim=0)) + self.examples\n",
    "        self.examples = self.examples[:self.max_len]\n",
    "        return inp_imgs\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_samples(model, inp_imgs, steps=60, step_size=10, return_img_per_step=False):\n",
    "        \"\"\"\n",
    "        Function for sampling images for a given model. \n",
    "        Inputs:\n",
    "            model - Neural network to use for modeling E_theta\n",
    "            inp_imgs - Images to start from for sampling. If you want to generate new images, enter noise between -1 and 1.\n",
    "            steps - Number of iterations in the MCMC algorithm.\n",
    "            step_size - Learning rate nu in the algorithm above\n",
    "            return_img_per_step - If True, we return the sample at every iteration of the MCMC\n",
    "        \"\"\"\n",
    "        # Before MCMC: set model parameters to \"required_grad=False\"\n",
    "        # because we are only interested in the gradients of the input. \n",
    "        is_training = model.training\n",
    "        model.eval()\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = False\n",
    "        inp_imgs.requires_grad = True\n",
    "        \n",
    "        # Enable gradient calculation if not already the case\n",
    "        had_gradients_enabled = torch.is_grad_enabled()\n",
    "        torch.set_grad_enabled(True)\n",
    "        \n",
    "        # We use a buffer tensor in which we generate noise each loop iteration.\n",
    "        # More efficient than creating a new tensor every iteration.\n",
    "        noise = torch.randn(inp_imgs.shape, device=inp_imgs.device)\n",
    "        \n",
    "        # List for storing generations at each step (for later analysis)\n",
    "        imgs_per_step = []\n",
    "        \n",
    "        # Loop over K (steps)\n",
    "        for _ in range(steps):\n",
    "            # Part 1: Add noise to the input.\n",
    "            noise.normal_(0, 0.005)\n",
    "            inp_imgs.data.add_(noise.data)\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "            \n",
    "            # Part 2: calculate gradients for the current input.\n",
    "            out_imgs = -model(inp_imgs)\n",
    "            out_imgs.sum().backward()\n",
    "            inp_imgs.grad.data.clamp_(-0.03, 0.03) # For stabilizing and preventing too high gradients\n",
    "\n",
    "            # Apply gradients to our current samples\n",
    "            inp_imgs.data.add_(-step_size * inp_imgs.grad.data)\n",
    "            inp_imgs.grad.detach_()\n",
    "            inp_imgs.grad.zero_()\n",
    "            inp_imgs.data.clamp_(min=-1.0, max=1.0)\n",
    "            \n",
    "            if return_img_per_step:\n",
    "                imgs_per_step.append(inp_imgs.clone().detach())\n",
    "        \n",
    "        # Reactivate gradients for parameters for training\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "        model.train(is_training)\n",
    "        \n",
    "        # Reset gradient calculation to setting before this function\n",
    "        torch.set_grad_enabled(had_gradients_enabled)\n",
    "\n",
    "        if return_img_per_step:\n",
    "            return torch.stack(imgs_per_step, dim=0)\n",
    "        else:\n",
    "            return inp_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suRpsXWzFlub"
   },
   "source": [
    "### 3. Training loop\n",
    "\n",
    "In the next section we define the training loop as indicated in the pseudocode above. The hyperparameters defined here are specific to the MNIST dataset and were fine-tuned by Philipp Lippe in the tutorial on which this notebook is based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "u6PR2ZMXE-1M"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code based on Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.0, 0.999))\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97) # Exponential decay over epochs\n",
    "\n",
    "def training_loop(cnn, train_loader, optimizer, scheduler, num_epochs = 60, alpha=0.1, img_shape=(1,28,28)):\n",
    "\n",
    "  sampler = Sampler(cnn, img_shape=img_shape, sample_size=train_loader.batch_size)\n",
    "\n",
    "  # Put model to train mode\n",
    "  cnn.train()\n",
    "  torch.set_grad_enabled(True)\n",
    "\n",
    "  for epochs in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 1. Sampling real data from the dataset\n",
    "        # (Add minimal noise to the original images to prevent the model from focusing on purely \"clean\" inputs)\n",
    "        real_imgs, _ = batch\n",
    "        small_noise = torch.randn_like(real_imgs) * 0.005\n",
    "        real_imgs.add_(small_noise).clamp_(min=-1.0, max=1.0)\n",
    "\n",
    "        # 2. Sampling fake data\n",
    "        fake_imgs = sampler.sample_new_exmps(steps=60, step_size=10)\n",
    "\n",
    "        # 3. calculate the contrastive divergence objective\n",
    "        inp_imgs = torch.cat([real_imgs, fake_imgs], dim=0)\n",
    "        real_out, fake_out = cnn(inp_imgs).chunk(2, dim=0)\n",
    "        cdiv_loss = fake_out.mean() - real_out.mean()\n",
    "\n",
    "        # 4. Add a regularization loss\n",
    "        reg_loss = (real_out ** 2 + fake_out ** 2).mean()\n",
    "      \n",
    "        # 5. Perform an update step with an optimizer on the combined loss\n",
    "        loss = alpha * reg_loss + cdiv_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsWyhXtnKoSC"
   },
   "source": [
    "To run the training, run the cell below. Consider having the necessary resources where you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "3VVtAV3eIhoe"
   },
   "outputs": [],
   "source": [
    "#training_loop(model, train_loader, optimizer, scheduler, num_epochs = 60, alpha=0.1, img_shape=(1,28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QABdS5DF4OEB"
   },
   "source": [
    "## Image Generation\n",
    "\n",
    "In this section we will sample images of the model to check if they are as realistic as the ones of MNIST dataset. This will show if we have managed to model the data distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "NIrYTjqh5Xo9",
    "outputId": "e1b2b16e-a4a6-4832-a730-fdf328749e76"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Code from Philipp Lippe's tutorial.\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html \n",
    "\"\"\"\n",
    "def generate_imgs(model, batch_size, num_steps):\n",
    "      start_imgs = torch.rand((batch_size,) + (1,28,28)).to(device)\n",
    "      start_imgs = start_imgs * 2 - 1\n",
    "      torch.set_grad_enabled(True)  # Tracking gradients for sampling necessary\n",
    "      imgs = Sampler.generate_samples(model, start_imgs, steps=num_steps, step_size=10, return_img_per_step=False)\n",
    "      torch.set_grad_enabled(False)\n",
    "      return imgs\n",
    "\n",
    "def printResults(imgs):\n",
    "    \"\"\" Displays sample images\n",
    "    \"\"\" \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "    axes[0].imshow(imgs[0][0,:,:], cmap='gray')\n",
    "    axes[1].imshow(imgs[1][0,:,:], cmap='gray')\n",
    "    axes[2].imshow(imgs[2][0,:,:], cmap='gray')\n",
    "    axes[3].imshow(imgs[3][0,:,:], cmap='gray')\n",
    "    fig.tight_layout()\n",
    "\n",
    "imgs = generate_imgs(model, batch_size=4, num_steps=256)\n",
    "printResults(imgs)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPnyRGUBiYwJe9m54Csmdjq",
   "include_colab_link": true,
   "name": "Energy based models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
